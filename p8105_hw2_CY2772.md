HW3
================
Chenhui Yan
2024-10-02

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
```

# Problem 1

## Part1: Data Cleaning:

- Retain variables: line, station, name, station latitude, longitude,
  routes served, entry, vending, entrance type, and ADA compliance.

- Convert entry to logical: The entry column has YES or NO values, so
  you’ll need to convert it into a logical TRUE/FALSE format. You can
  use functions like ifelse() or case_when() to accomplish t

``` r
# Read and clean the data
nyc_subway_data = read_csv("./dataset_hw3/hw2_prob1_dataset.csv") %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) %>% 
  mutate(entry = case_when(
    entry == "YES" ~ TRUE,
    entry == "NO"  ~ FALSE
  )) # Convert entry to logical TRUE/FALSE
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# View the cleaned data
nyc_subway_data
```

    ## # A tibble: 1,868 × 19
    ##    line     station_name station_latitude station_longitude route1 route2 route3
    ##    <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ##  1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  6 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  7 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  8 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  9 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ## 10 4 Avenue 53rd St                  40.6             -74.0 R      <NA>   <NA>  
    ## # ℹ 1,858 more rows
    ## # ℹ 12 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <dbl>, route9 <dbl>, route10 <dbl>, route11 <dbl>, entry <lgl>,
    ## #   vending <chr>, entrance_type <chr>, ada <lgl>

## Part2: A brief summary of the dataset:

The cleaned dataset contains variables such as subway line, station
name, latitude/longitude, routes served, entry status, vending, entrance
type, and ADA compliance. To clean the data, I standardized column names
using clean_names(), selected only relevant columns with select(), and
converted the entry variable from character (“YES”/“NO”) to logical
(TRUE/FALSE) using case_when(). The resulting dataset has 1868 rows and
19 columns, and it is tidy since each variable is a column, each
observation is a row, and each value is a cell.

## Part3: Answer the following questions using these data:

### How many distinct stations are there?

``` r
distinct_station = 
   nyc_subway_data %>%
  distinct(line, station_name) %>%
  nrow()
```

There are 465 distinct stations in this dataset.

### How many stations are ADA compliant?

``` r
ada_compliant =
  nyc_subway_data %>% 
  filter(ada == TRUE) %>% 
  distinct(line, station_name) %>% 
  nrow()
```

There are 84 ADA compliant stations.

### What proportion of station entrances / exits without vending allow entrance?

``` r
proportion_no_vending_allows_entry = 
  nyc_subway_data %>%
  filter(vending == "NO") %>%  # Filter rows where there is no vending
  summarise(proportion = mean(entry == TRUE)) %>%  # Calculate the proportion where entry is TRUE
  pull(proportion)

# Output the result
proportion_no_vending_allows_entry
```

    ## [1] 0.3770492

37.7% of station entrances / exits without vending allow entrance

## Part4: Reformat data so that route number and route name are distinct variables.

``` r
nyc_subway_data_refored = 
  nyc_subway_data %>%
  mutate(across(starts_with("route"), as.character)) %>%  # Ensure all route columns are characters
  pivot_longer(
    cols = starts_with("route"), # Specify the columns for the routes
    names_to = "route_number",
    values_to = "route") %>%
  drop_na(route)# Drop rows where route is NA
nyc_subway_data_refored
```

    ## # A tibble: 4,270 × 10
    ##    line     station_name station_latitude station_longitude entry vending
    ##    <chr>    <chr>                   <dbl>             <dbl> <lgl> <chr>  
    ##  1 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ##  2 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ##  3 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  4 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  5 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  6 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  7 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  8 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  9 4 Avenue 45th St                  40.6             -74.0 TRUE  YES    
    ## 10 4 Avenue 45th St                  40.6             -74.0 TRUE  YES    
    ## # ℹ 4,260 more rows
    ## # ℹ 4 more variables: entrance_type <chr>, ada <lgl>, route_number <chr>,
    ## #   route <chr>

### How many distinct stations serve the A train?\*

``` r
nyc_subway_data_refored %>% 
  filter(route == "A") %>% 
  distinct(line, station_name) %>% 
  nrow()
```

    ## [1] 60

There are distinct stations serve the A train.

### How many are ADA compliant of the stations that serve the A train?\*

``` r
nyc_subway_data_refored %>% 
  filter(ada == TRUE) %>%  
  filter(route == "A") %>% 
  distinct(line, station_name) %>% 
  nrow()
```

    ## [1] 17

Of the 60 stations that serve the A train,17 have at least one ADA
compliant entry or exit.

# Problem 2

## Part1. Data Cleaning for Mr trash wheel sheet

``` r
# Specify sheet and omit non-data entries (skip rows and specify range)
mr_trash_wheel = read_excel("./dataset_hw3/hw2_prob2_datas.xlsx",
                            sheet = "Mr. Trash Wheel", 
                            skip = 1,  # Skip the first row if it's non-data or a note
                            range = "A2:N653")  # Adjust the range based on the actual data layout
# Omit rows that do not include dumpster-specific data
mr_trash_wheel_cleaned = mr_trash_wheel %>%
  filter(!is.na(Dumpster)) %>%  # Keep only rows where the 'dumpster' column is not NA
  janitor::clean_names() %>%
  select(dumpster:homes_powered) %>% 
  mutate(sports_balls = round(sports_balls, 0)) %>% 
  mutate(sports_balls = as.integer(sports_balls)) %>% 
  mutate(year = as.numeric(year)) %>% 
  mutate(trash_wheel = "mr_trash")
mr_trash_wheel_cleaned
```

    ## # A tibble: 651 × 15
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 641 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

## Part2: Data cleaning for Professor Trash Wheel sheet

``` r
pro_trash_wheel = 
  read_excel("./dataset_hw3/hw2_prob2_datas.xlsx",
             sheet = "Professor Trash Wheel",
             skip = 1,
             range = "A2:M121",) %>%
  janitor::clean_names() %>% 
  select(dumpster:homes_powered) %>%
  mutate(year = as.numeric(year)) %>% 
  mutate(trash_wheel = "pro_trash")

pro_trash_wheel 
```

    ## # A tibble: 119 × 14
    ##    dumpster month     year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>    <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 January   2017 2017-01-02 00:00:00        1.79                 15
    ##  2        2 January   2017 2017-01-30 00:00:00        1.58                 15
    ##  3        3 February  2017 2017-02-26 00:00:00        2.32                 18
    ##  4        4 February  2017 2017-02-26 00:00:00        3.72                 15
    ##  5        5 February  2017 2017-02-28 00:00:00        1.45                 15
    ##  6        6 March     2017 2017-03-30 00:00:00        1.71                 15
    ##  7        7 April     2017 2017-04-01 00:00:00        1.82                 15
    ##  8        8 April     2017 2017-04-20 00:00:00        2.37                 15
    ##  9        9 May       2017 2017-05-10 00:00:00        2.64                 15
    ## 10       10 May       2017 2017-05-26 00:00:00        2.78                 15
    ## # ℹ 109 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>, trash_wheel <chr>

## Part3: Data cleaning for Gwynnda sheet

``` r
gwynnda_df = 
  read_excel("./dataset_hw3/hw2_prob2_datas.xlsx", 
             sheet = "Gwynnda Trash Wheel",
             skip = 1,
             range = "A2:L265", ) %>% 
  janitor::clean_names() %>% 
  select(dumpster:homes_powered) %>%
  mutate(trash_wheel = "gwynnda_trash") %>% 
  mutate(year = as.numeric(year))
gwynnda_df
```

    ## # A tibble: 263 × 13
    ##    dumpster month   year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>  <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 July    2021 2021-07-03 00:00:00        0.93                 15
    ##  2        2 July    2021 2021-07-07 00:00:00        2.26                 15
    ##  3        3 July    2021 2021-07-07 00:00:00        1.62                 15
    ##  4        4 July    2021 2021-07-16 00:00:00        1.76                 15
    ##  5        5 July    2021 2021-07-30 00:00:00        1.53                 15
    ##  6        6 August  2021 2021-08-11 00:00:00        2.06                 15
    ##  7        7 August  2021 2021-08-14 00:00:00        1.9                  15
    ##  8        8 August  2021 2021-08-16 00:00:00        2.16                 15
    ##  9        9 August  2021 2021-08-16 00:00:00        2.6                  15
    ## 10       10 August  2021 2021-08-17 00:00:00        3.21                 15
    ## # ℹ 253 more rows
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>, trash_wheel <chr>

## Part4: Combined Data

``` r
combined_trash_wheel_data = bind_rows(mr_trash_wheel_cleaned,
                                       pro_trash_wheel,
                                       gwynnda_df)
combined_trash_wheel_data
```

    ## # A tibble: 1,033 × 15
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 1,023 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

## Part5: A paragraph about these data

``` r
# Number of observations in the combined dataset
total_observations = nrow(combined_trash_wheel_data)

# Total weight of trash collected by Professor Trash Wheel
total_weight_prof_trash_wheel = combined_trash_wheel_data %>%
  filter(trash_wheel == "pro_trash") %>%
  summarise(total_weight = sum(weight_tons, na.rm = TRUE)) %>%
  pull(total_weight)

# Total number of cigarette butts collected by Gwynnda in June 2022
total_cig_butts_gwynnda_june_2022 = combined_trash_wheel_data %>%
  filter(trash_wheel == "gwynnda_trash", month == "June", year == 2022) %>%
  summarise(total_cig_butts = sum(cigarette_butts, na.rm = TRUE)) %>%
  pull(total_cig_butts)

# Print the results
total_observations
```

    ## [1] 1033

``` r
total_weight_prof_trash_wheel
```

    ## [1] 246.74

``` r
total_cig_butts_gwynnda_june_2022
```

    ## [1] 18120

I combined the data from Mr. Trash Wheel, Professor Trash Wheel, and
Gwynnda into a single dataset called combined_trash_wheel_data, which
consists of 1,033 observations. This combined dataset captures
information on the weight (in tons) and volume (in cubic yards) of trash
removed by each trash wheel from Baltimore’s Inner Harbor between May
2016 and June 2023. It also includes details on the various types of
trash collected, such as plastic bottles, cigarette butts, and sports
balls, along with the quantities removed by each trash wheel during this
period. Additionally, the homes_powered variable indicates the number of
homes powered per ton of trash collected.

In total, Professor Trash Wheel collected 246.74 tons of trash. Gwynnda
collected 18,120 cigarette butts in June 2022. \# Problem 3 \## Part1:
Data Wrangling

``` r
# Clean and separate baker names, handling cases where bakers have multiple names
bakers_df =
  read_csv("./dataset_hw3/gbb_datasets/bakers.csv", 
           na = c("NA", "", ".")) %>%
  janitor::clean_names() %>%
  separate(baker_name, into = c("first_name", "last_name"), 
           sep = " ", 
           extra = "merge",  # This ensures that middle names are included in 'last_name'
           fill = "right")  # If there's no last name, it'll fill it with NA
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Load and clean bakes data
bakes_df = read_csv("./dataset_hw3/gbb_datasets/bakes.csv", 
                    na = c("NA", "", ".")) %>%
  janitor::clean_names()
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Load and clean results data, skipping 2 rows if needed
results_df = read_csv("./dataset_hw3/gbb_datasets/results.csv", 
                      skip = 2, 
                      na = c("NA", "", ".")) %>%
  janitor::clean_names()
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Check column names in both datasets
colnames(bakes_df)
```

    ## [1] "series"         "episode"        "baker"          "signature_bake"
    ## [5] "show_stopper"

``` r
colnames(bakers_df)
```

    ## [1] "first_name"       "last_name"        "series"           "baker_age"       
    ## [5] "baker_occupation" "hometown"

``` r
colnames(results_df)
```

    ## [1] "series"    "episode"   "baker"     "technical" "result"

## Part2: Clean each dataset:

- Check for missing values: Use summary() or is.na() in R to check for
  missing data.
- Check for consistency across datasets: Use anti_join() in R to detect
  discrepancies between datasets (e.g., missing matches between bakers
  and results).

### Step 1: Use anti_join() to check dataset matching and completeness.

``` r
# Combine first_name and last_name into a single 'baker' column in bakers_df
bakers_df = bakers_df %>%
  mutate(baker = paste(first_name, last_name))

# Check for unmatched rows using correct 'by' syntax
unmatched_bakes = anti_join(bakes_df, bakes_df, by = c("baker", "series"))
unmatched_results = anti_join(results_df, bakers_df, by = c("baker", "series"))
unmatched_bakers = anti_join(bakers_df, results_df, by = c("baker", "series"))

# View unmatched rows (if any)
unmatched_bakes
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker <chr>,
    ## #   signature_bake <chr>, show_stopper <chr>

``` r
unmatched_results
```

    ## # A tibble: 1,136 × 5
    ##    series episode baker     technical result
    ##     <dbl>   <dbl> <chr>         <dbl> <chr> 
    ##  1      1       1 Annetha           2 IN    
    ##  2      1       1 David             3 IN    
    ##  3      1       1 Edd               1 IN    
    ##  4      1       1 Jasminder        NA IN    
    ##  5      1       1 Jonathan          9 IN    
    ##  6      1       1 Louise           NA IN    
    ##  7      1       1 Miranda           8 IN    
    ##  8      1       1 Ruth             NA IN    
    ##  9      1       1 Lea              10 OUT   
    ## 10      1       1 Mark             NA OUT   
    ## # ℹ 1,126 more rows

``` r
unmatched_bakers
```

    ## # A tibble: 120 × 7
    ##    first_name last_name   series baker_age baker_occupation       hometown baker
    ##    <chr>      <chr>        <dbl>     <dbl> <chr>                  <chr>    <chr>
    ##  1 Ali        Imdad            4        25 Charity worker         Saltley… Ali …
    ##  2 Alice      Fevronia        10        28 Geography teacher      Essex    Alic…
    ##  3 Alvin      Magallanes       6        37 Nurse                  Brackne… Alvi…
    ##  4 Amelia     LeBruin         10        24 Fashion designer       Halifax  Amel…
    ##  5 Andrew     Smyth            7        25 Aerospace engineer     Derby /… Andr…
    ##  6 Annetha    Mills            1        30 Midwife                Essex    Anne…
    ##  7 Antony     Amourdoux        9        30 Banker                 London   Anto…
    ##  8 Beca       Lyne-Pirkis      4        31 Military Wives' Choir… Aldersh… Beca…
    ##  9 Ben        Frazer           2        31 Graphic Designer       Northam… Ben …
    ## 10 Benjamina  Ebuehi           7        23 Teaching assistant     South L… Benj…
    ## # ℹ 110 more rows

### Step 2: Standardize Baker Names (Resolve “Joanne” to “Jo”)

The anti_join() result showed that the names “Joanne” and “Jo” do not
match, although they both belong to the same series. We assume they
refer to the same person and will standardize the name to “Jo” across
the datasets.

``` r
# Standardize the 'baker' name from 'Joanne' to 'Jo' in results_df
results_df = results_df %>%
  mutate(baker = ifelse(baker == "Joanne", "Jo", baker))

# Remove any extra quotes around the name 'Jo' in bakes_df if necessary
bakes_df = bakes_df %>%
  mutate(baker = ifelse(baker == '"Jo"', "Jo", baker))  # Standardize 'Jo' format
```

### Step 3: Verify Completeness After Correcting Baker Names

After standardizing the names (e.g., renaming “Joanne” to “Jo”), we will
recheck the datasets to ensure that there are no remaining mismatches
between the bakes_df, results_df, and bakers_df datasets.

``` r
# Recheck for any unmatched rows after correcting the names
unmatched_bakes = anti_join(bakes_df, bakers_df, by = c("baker", "series"))
unmatched_results = anti_join(results_df, bakers_df, by = c("baker", "series"))
unmatched_bakers = anti_join(bakers_df, results_df, by = c("baker", "series"))

# Display unmatched rows (if any exist)
unmatched_bakes
```

    ## # A tibble: 548 × 5
    ##    series episode baker     signature_bake                          show_stopper
    ##     <dbl>   <dbl> <chr>     <chr>                                   <chr>       
    ##  1      1       1 Annetha   "Light Jamaican Black Cakewith Strawbe… Red, White …
    ##  2      1       1 David     "Chocolate Orange Cake"                 Black Fores…
    ##  3      1       1 Edd       "Caramel Cinnamon and Banana Cake"      N/A         
    ##  4      1       1 Jasminder "Fresh Mango and Passion Fruit Humming… N/A         
    ##  5      1       1 Jonathan  "Carrot Cake with Lime and Cream Chees… Three Tiere…
    ##  6      1       1 Lea       "Cranberry and Pistachio Cakewith Oran… Raspberries…
    ##  7      1       1 Louise    "Carrot and Orange Cake"                Never Fail …
    ##  8      1       1 Mark      "Sticky Marmalade Tea Loaf"             Heart-shape…
    ##  9      1       1 Miranda   "Triple Layered Brownie Meringue Cake\… Three Tiere…
    ## 10      1       1 Ruth      "Three Tiered Lemon Drizzle Cakewith F… Classic Cho…
    ## # ℹ 538 more rows

``` r
unmatched_results
```

    ## # A tibble: 1,136 × 5
    ##    series episode baker     technical result
    ##     <dbl>   <dbl> <chr>         <dbl> <chr> 
    ##  1      1       1 Annetha           2 IN    
    ##  2      1       1 David             3 IN    
    ##  3      1       1 Edd               1 IN    
    ##  4      1       1 Jasminder        NA IN    
    ##  5      1       1 Jonathan          9 IN    
    ##  6      1       1 Louise           NA IN    
    ##  7      1       1 Miranda           8 IN    
    ##  8      1       1 Ruth             NA IN    
    ##  9      1       1 Lea              10 OUT   
    ## 10      1       1 Mark             NA OUT   
    ## # ℹ 1,126 more rows

``` r
unmatched_bakers
```

    ## # A tibble: 120 × 7
    ##    first_name last_name   series baker_age baker_occupation       hometown baker
    ##    <chr>      <chr>        <dbl>     <dbl> <chr>                  <chr>    <chr>
    ##  1 Ali        Imdad            4        25 Charity worker         Saltley… Ali …
    ##  2 Alice      Fevronia        10        28 Geography teacher      Essex    Alic…
    ##  3 Alvin      Magallanes       6        37 Nurse                  Brackne… Alvi…
    ##  4 Amelia     LeBruin         10        24 Fashion designer       Halifax  Amel…
    ##  5 Andrew     Smyth            7        25 Aerospace engineer     Derby /… Andr…
    ##  6 Annetha    Mills            1        30 Midwife                Essex    Anne…
    ##  7 Antony     Amourdoux        9        30 Banker                 London   Anto…
    ##  8 Beca       Lyne-Pirkis      4        31 Military Wives' Choir… Aldersh… Beca…
    ##  9 Ben        Frazer           2        31 Graphic Designer       Northam… Ben …
    ## 10 Benjamina  Ebuehi           7        23 Teaching assistant     South L… Benj…
    ## # ℹ 110 more rows

## Part 3: Merge the datasets

To combine the results_df, bakes_df, and bakers_df datasets into a
single unified dataset, we perform successive left_join() operations.
The resulting dataset is then reorganized and saved as a CSV file.

``` r
# Merge results_df, bakes_df, and bakers_df into one final dataset
final_df <- results_df %>%
  left_join(bakes_df, by = c("baker", "series", "episode")) %>%
  left_join(bakers_df, by = c("baker", "series")) %>%
  relocate(series, episode, .before = "baker")  # Reorder columns for better readability

# Export the final dataset as a CSV file
write.csv(final_df, file = "./dataset_hw3/gbb_datasets/final.csv")
```

## Description

Upon reviewing the raw data, I found that while bakers.csv contains the
full names of the bakers, both bakes.csv and results.csv only list the
first names. To address this, I split the full names in bakers.csv into
first and last names. Additionally, I noticed that the first two rows of
results.csv contained unnecessary information, so I skipped them to
simplify processing.

Next, I used the anti_join function to identify any inconsistencies
between the datasets and resolved the mismatches, particularly with the
baker names. After ensuring all data was aligned, I merged the datasets
in a way that retained all relevant information. I also reorganized the
variables for better clarity, placing the series and episode information
before the baker’s name.

Finally, I saved the merged dataset in the gbb_dataset directory. The
resulting dataset combines all the information from bakers.csv,
bakes.csv, and results.csv without losing any data. The variables are
arranged logically, making it easy for users to quickly locate bakers
based on the series and episode.

## Part 4: Create table showing winner or star baker

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10.

``` r
# Filter results for seasons 5 to 10 and include only winners and star bakers
winners_df <- results_df %>%
  filter(between(series, 5, 10), result %in% c("WINNER", "STAR BAKER")) %>%
  select(series, episode, baker, result)

# Pivot the data and arrange by episode, then display it in a table format
winners_df %>%
  pivot_wider(names_from = series, values_from = baker) %>%
  arrange(episode) %>%
  knitr::kable()
```

| episode | result     | 5       | 6      | 7         | 8      | 9       | 10       |
|--------:|:-----------|:--------|:-------|:----------|:-------|:--------|:---------|
|       1 | STAR BAKER | Nancy   | Marie  | Jane      | Steven | Manon   | Michelle |
|       2 | STAR BAKER | Richard | Ian    | Candice   | Steven | Rahul   | Alice    |
|       3 | STAR BAKER | Luis    | Ian    | Tom       | Julia  | Rahul   | Michael  |
|       4 | STAR BAKER | Richard | Ian    | Benjamina | Kate   | Dan     | Steph    |
|       5 | STAR BAKER | Kate    | Nadiya | Candice   | Sophie | Kim-Joy | Steph    |
|       6 | STAR BAKER | Chetna  | Mat    | Tom       | Liam   | Briony  | Steph    |
|       7 | STAR BAKER | Richard | Tamal  | Andrew    | Steven | Kim-Joy | Henry    |
|       8 | STAR BAKER | Richard | Nadiya | Candice   | Stacey | Ruby    | Steph    |
|       9 | STAR BAKER | Richard | Nadiya | Andrew    | Sophie | Ruby    | Alice    |
|      10 | WINNER     | Nancy   | Nadiya | Candice   | Sophie | Rahul   | David    |

From the results, we observe that each episode of every season features
a baker being awarded “STAR BAKER,” and at the end of each season, a
“WINNER” is crowned. It seems logical to assume that the more often a
baker is named “STAR BAKER,” the higher their chances of becoming the
season’s “WINNER.” This pattern holds for Nadiya in season 6, Candice in
season 7, Sophie in season 8, and Rahul in season 9. However, the
winners of seasons 5 and 10 did not follow this trend. \## Part 5:
Import the views dataset

``` r
# Load and clean viewership data, reshaping it for analysis
viewership_df =
  read_csv("./dataset_hw3/gbb_datasets/viewers.csv") %>%
  janitor::clean_names() %>%
  gather(key = "series", value = "viewership", series_1:series_10) %>%
  mutate(
    series = as.numeric(gsub("series_", "", series))  # Extract series number and convert to numeric
  ) %>%
  arrange(series) %>%
  select(series, everything())  # Ensure 'series' is the first column
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
knitr::kable(head(viewership_df, 10), caption = "First 10 rows in the Viewership dataset")
```

| series | episode | viewership |
|-------:|--------:|-----------:|
|      1 |       1 |       2.24 |
|      1 |       2 |       3.00 |
|      1 |       3 |       3.00 |
|      1 |       4 |       2.60 |
|      1 |       5 |       3.03 |
|      1 |       6 |       2.75 |
|      1 |       7 |         NA |
|      1 |       8 |         NA |
|      1 |       9 |         NA |
|      1 |      10 |         NA |

First 10 rows in the Viewership dataset

I imported the data, cleaned the variable names, and organized it using
pivot_longer. The dataset now has a series column representing each
season and a viewership column containing the corresponding viewership
data. I removed the “series\_” prefix to convert the series numbers into
numeric values and then arranged the data in ascending order by series.
The first ten rows of this dataset are shown above. The average
viewership in series 1 is 2.77. The average viewership in series 5 is
10.04.
